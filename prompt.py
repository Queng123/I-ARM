import logging
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.prompts.chat import (ChatPromptTemplate,
                                    HumanMessagePromptTemplate,
                                    SystemMessagePromptTemplate)
from langchain.vectorstores import Chroma
from langchain.schema import HumanMessage, SystemMessage, AIMessage
import constants

# Import conditionnel selon si on est sur les API officielles d'OpenAI ou sur Azure.
if constants.OPENAI_API_TYPE == "azure":
    from langchain.chat_models import AzureChatOpenAI as Chat
else:
    from langchain.chat_models import ChatOpenAI as Chat

# Configure le logger
logging.basicConfig(format='%(levelname)s: %(message)s',
                    encoding='utf-8', level=logging.INFO)

# template du prompt Syst√®me
SYSTEM_PROMPT = """
Tu es un ARM. Assistant m√©dical de r√©gulation. Ton r√¥le est de r√©pondre au mieux aux urgences des demandeurs.
Voici tes missions :
RM1 . recueil des donn√©es administratives de l'appelant: num√©ro de t√©l√©phone, adresse compl√®te, identit√© (appelant,
patient) ...
ARM2. √©coute de la demande, dialogue empathique et qualification de l'appel.
ARM3. recherche de signes d'une d√©tresse vitale (et engagement r√©flexe √©ventuel des moyens).
ARM4. priorisation de la r√©gulation m√©dicale.
ARMS. transmission d'une synth√®se ()motif de recours) au m√©decin r√©gulateur.
MR1. phase de reconnaissance sur la base de la synth√®se effectu√©e par l'ARM.
MR2, interrogatoire m√©dical: analyse du contexte, des demandes et attentes du patient; recherche des ant√©c√©dents
m√©dicaux, des traitements, des facteurs de risque; qualification du besoin de soins; si n√©cessaire, hypoth√®ses
diagnostiques ou pronostiques.
MR3. estimation des risques de la situation en regard des b√©n√©fices attendus des diff√©rentes prises en charge possibles;
respect du libre choix du patient; d√©cision : choix des moyens adapt√©s et √©ventuellement leur composition.
MR4. information de l'appelant et contrat de soins; mise en ≈ìuvre d'une √©ventuelle prescription.
demander √† l'appelant de laisser libre la ligne t√©l√©phonique.
ARM6. d√©clenchement des moyens, mise en ≈ìuvre des d√©cisions du m√©decin r√©gulateur.
MRS. conseils en attendant l'arriv√©e des secours; anticipation de chacune des √©tapes de la prise en charge du patient.
ARM7. suivi de l'intervention ; r√©ception des messages secouristes ; conseils √©ventuels; rappel syst√©matique une heure
apr√®s dispensation d'un conseil m√©dical se r√©f√©rer √† la proc√©dure ¬´ rappel ¬ª).
MR6. transmission de l'information m√©dicale utile aux intervenants.
ARM8. recherche de lits d'aval si besoin.
MR7. suivi de l'intervention, r√©ception des messages m√©dicaux et parfois secouristes, conseils et assistance aux √©ventuels
intervenants, codification de la partie r√©gulation m√©dicale.
ARM9. recherche de compl√©ments d'information administrative, finalisation du dossier (codification de la partie
qualification, √©ditions √©ventuelles ...).
MR8. si besoin: pr√©paration de l'accueil dans un √©tablissement de soins; recueil de l'accord du service receveur; respect
du libre choix du patient.
ARM10. cl√¥ture du dossier administratif.
MR9. cl√¥ture du dossier m√©dical.
MR10. cl√¥ture du dossier de r√©gulation m√©dicale (DRM).
Introduit toi de la mani√®re suivante : Bonjour, ici le Samu, Je suis l√† pour vous aider. O√π vous trouvez-vous ?
Ne pose pas des questions sur plusieurs th√©matiques en m√™me temps pour rendre la discussion plus naturelle.
Si l'urgence est av√©r√©, transfert directement l'appelant au m√©decin r√©gulateur.
Tu ne doit JAMAIS dire ¬´ mais il est important de noter que ¬ª ou ¬´ en tant que mod√®le de langage d'IA ¬ª.
===========
Niveau d'urgence :
{urgency}
===========
CONTEXTE:
{context}
===========
"""

CHAT = Chat(model_name=constants.MODEL_NAME, temperature=constants.TEMPERATURE)
DB = Chroma(persist_directory=constants.DATABASE_DIR,
            embedding_function=OpenAIEmbeddings(
                deployment=constants.OPENAI_DEPLOYMENT_NAME,
                model=constants.EMBEDDINGS_MODEL,
                openai_organization=constants.OPENAI_ORGANIZATION,
                openai_api_base=constants.OPENAI_API_BASE,
                openai_api_version=constants.OPENAI_API_VERSION))

# Liste pour stocker l'historique des messages
message_history = []

def format_context(results):
    """Formate les r√©sultats de la recherche pour les inclure dans le contexte."""
    context = ""
    for result in results:
        context += f"{result.page_content}\n"
    return context

def ask_question(question, urgency):
    """Poser une question au mod√®le."""

    # Cherche des chunks de textes similaires √† la question
    results = DB.as_retriever(search_type="similarity", search_kwargs={'k': 5}).get_relevant_documents(query=question)

    logging.info("Sources: %s", results)

    # Formate le contexte √† partir des r√©sultats
    context = format_context(results)

    # Constitue la s√©quence de chat avec le conditionnement du bot et la question de l'utilisateur
    system_message_prompt = SystemMessagePromptTemplate.from_template(SYSTEM_PROMPT.format(urgency=urgency, context=context))
    human_message_prompt = HumanMessagePromptTemplate.from_template("QUESTION: {question}")
    chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])

    # Ajouter l'historique des messages
    messages = message_history + chat_prompt.format_prompt(context=context, question=question).to_messages()

    # Log messages pour d√©bogage
    logging.info("Messages: %s", messages)

    # Pose la question au LLM
    response = CHAT(messages).content

    # Ajouter la question et la r√©ponse √† l'historique
    message_history.extend(messages)
    message_history.append(AIMessage(content=response))
    logging.info("R√©ponse: %s", response)
    return response, results

def sentiment_analysis(question):
    """Analyser le sentiment d'une question avec le mod√®le."""

    # Constitue la s√©quence de chat avec le conditionnement du bot et la question
    # de l'utilisateur
    system_message_prompt = SystemMessagePromptTemplate.from_template(
        "Vous √™tes un mod√®le de langage form√© par OpenAI. Votre t√¢che est d'analyser le sentiment du texte suivant :")
    human_message_prompt = HumanMessagePromptTemplate.from_template(
        "{question}")
    chat_prompt = ChatPromptTemplate.from_messages(
        [system_message_prompt, human_message_prompt]
    )
    messages = chat_prompt.format_prompt(
        question=question
    ).to_messages()

    # Pose la question au LLM
    response = CHAT(messages).content
    logging.info("Sentiment: %s", response)
    return response

def categorize_urgency(question):
    """Cat√©gorise l'urgence d'un texte en fonction de sa similarit√© avec les descriptions des niveaux d'urgence."""
    # Constitue la s√©quence de chat avec le conditionnement du bot et la question
    # de l'utilisateur
    system_message_prompt = SystemMessagePromptTemplate.from_template(
        "Vous √™tes un mod√®le de langage form√© par OpenAI. Votre t√¢che est d'analyser le sentiment pour d√©terminer le niveau d'urgence sur une √©chelle de 1 √† 5 : Niveau 1 : Urgence vitale, Niveau 2 : Urgence absolue, Niveau 3 : Urgence relative, Niveau 4 : Consultation m√©dicale urgente, Niveau 5 : Consultation non urgente, Niveau 6 : Fausse alerte. Commence ta r√©ponse par le niveau identifi√©")
    human_message_prompt = HumanMessagePromptTemplate.from_template(
        "{question}")
    chat_prompt = ChatPromptTemplate.from_messages(
        [system_message_prompt, human_message_prompt]
    )
    messages = chat_prompt.format_prompt(
        question=question
    ).to_messages()

    # Pose la question au LLM
    response = CHAT(messages).content
    logging.info("Urgency: %s", response)
    return response



if __name__ == "__main__":
    print("""
üë®‚Äçüíºüë©‚Äçüíº ARM üí¨

Posez simplement votre question:
""")
    print("----------------------------------------------------")
    print("üí¨ Votre question: ")

    while True:
        question = input()
        if question.lower() in ["exit", "quit"]:
            break

        answer, sources = ask_question(question)

        print("----------------------------------------------------")
        print("ü§ñ R√©ponse de l'expert: ")
        print(answer)
        print("----------------------------------------------------")
        print("üìö Documents utilis√©s: ")
        unique_sources = set()
        for source in sources:
            unique_sources.add((source.metadata['source'], source.metadata['page']))

        for source in unique_sources:
            print(f"- Document: {source[0]}, Page: {source[1]}")
        print("----------------------------------------------------")
        print("Posez une autre question ou tapez 'exit' pour quitter.")
