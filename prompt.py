import logging
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.prompts.chat import (ChatPromptTemplate,
                                    HumanMessagePromptTemplate,
                                    SystemMessagePromptTemplate)
from langchain.vectorstores import Chroma
from langchain.schema import HumanMessage, SystemMessage, AIMessage
import constants

# Import conditionnel selon si on est sur les API officielles d'OpenAI ou sur Azure.
if constants.OPENAI_API_TYPE == "azure":
    from langchain.chat_models import AzureChatOpenAI as Chat
else:
    from langchain.chat_models import ChatOpenAI as Chat

# Configure le logger
logging.basicConfig(format='%(levelname)s: %(message)s',
                    encoding='utf-8', level=logging.INFO)

# template du prompt Syst√®me
SYSTEM_PROMPT = """
Tu es un ARM. Assistant m√©dical de r√©gulation. Ton r√¥le est de r√©pondre au mieux aux urgences des demandeurs.
Tu ne doit JAMAIS dire ¬´ mais il est important de noter que ¬ª ou ¬´ en tant que mod√®le de langage d'IA ¬ª.

===========
CONTEXTE:
{context}
===========
"""

CHAT = Chat(model_name=constants.MODEL_NAME, temperature=constants.TEMPERATURE)
DB = Chroma(persist_directory=constants.DATABASE_DIR,
            embedding_function=OpenAIEmbeddings(
                deployment=constants.OPENAI_DEPLOYMENT_NAME,
                model=constants.EMBEDDINGS_MODEL,
                openai_organization=constants.OPENAI_ORGANIZATION,
                openai_api_base=constants.OPENAI_API_BASE,
                openai_api_version=constants.OPENAI_API_VERSION))

# Liste pour stocker l'historique des messages
message_history = []

def format_context(results):
    """Formate les r√©sultats de la recherche pour les inclure dans le contexte."""
    context = ""
    for result in results:
        context += f"{result.page_content}\n"
    return context

def ask_question(question):
    """Poser une question au mod√®le."""

    # Cherche des chunks de textes similaires √† la question
    results = DB.as_retriever(search_type="similarity", search_kwargs={'k': 5}).get_relevant_documents(query=question)

    logging.debug("Sources: %s", results)

    # Formate le contexte √† partir des r√©sultats
    context = format_context(results)

    # Constitue la s√©quence de chat avec le conditionnement du bot et la question de l'utilisateur
    system_message_prompt = SystemMessagePromptTemplate.from_template(SYSTEM_PROMPT)
    human_message_prompt = HumanMessagePromptTemplate.from_template("QUESTION: {question}")
    chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])

    # Ajouter l'historique des messages
    messages = message_history + chat_prompt.format_prompt(context=context, question=question).to_messages()

    # Log messages pour d√©bogage
    logging.debug("Messages: %s", messages)

    # Pose la question au LLM
    response = CHAT(messages).content

    # Ajouter la question et la r√©ponse √† l'historique
    message_history.extend(messages)
    message_history.append(AIMessage(content=response))

    return response, results

def sentiment_analysis(question):
    """Analyser le sentiment d'une question avec le mod√®le."""

    # Constitue la s√©quence de chat avec le conditionnement du bot et la question
    # de l'utilisateur
    system_message_prompt = SystemMessagePromptTemplate.from_template(
        "Vous √™tes un mod√®le de langage form√© par OpenAI. Votre t√¢che est d'analyser le sentiment du texte suivant :")
    human_message_prompt = HumanMessagePromptTemplate.from_template(
        "{question}")
    chat_prompt = ChatPromptTemplate.from_messages(
        [system_message_prompt, human_message_prompt]
    )
    messages = chat_prompt.format_prompt(
        question=question
    ).to_messages()

    # Pose la question au LLM
    response = CHAT(messages).content

    return response

def categorize_urgency(question):
    """Cat√©gorise l'urgence d'un texte en fonction de sa similarit√© avec les descriptions des niveaux d'urgence."""
    # Constitue la s√©quence de chat avec le conditionnement du bot et la question
    # de l'utilisateur
    system_message_prompt = SystemMessagePromptTemplate.from_template(
        "Vous √™tes un mod√®le de langage form√© par OpenAI. Votre t√¢che est d'analyser le sentiment pour d√©terminer le niveau d'urgence sur une √©chelle de 1 √† 5 : Niveau 1 : Urgence vitale, Niveau 2 : Urgence absolue, Niveau 3 : Urgence relative, Niveau 4 : Consultation m√©dicale urgente, Niveau 5 : Consultation non urgente. Commence ta r√©ponse par le niveau identifi√©")
    human_message_prompt = HumanMessagePromptTemplate.from_template(
        "{question}")
    chat_prompt = ChatPromptTemplate.from_messages(
        [system_message_prompt, human_message_prompt]
    )
    messages = chat_prompt.format_prompt(
        question=question
    ).to_messages()

    # Pose la question au LLM
    response = CHAT(messages).content

    return response



if __name__ == "__main__":
    print("""
üë®‚Äçüíºüë©‚Äçüíº ARM üí¨

Posez simplement votre question:
""")
    print("----------------------------------------------------")
    print("üí¨ Votre question: ")

    while True:
        question = input()
        if question.lower() in ["exit", "quit"]:
            break

        answer, sources = ask_question(question)

        print("----------------------------------------------------")
        print("ü§ñ R√©ponse de l'expert: ")
        print(answer)
        print("----------------------------------------------------")
        print("üìö Documents utilis√©s: ")
        unique_sources = set()
        for source in sources:
            unique_sources.add((source.metadata['source'], source.metadata['page']))

        for source in unique_sources:
            print(f"- Document: {source[0]}, Page: {source[1]}")
        print("----------------------------------------------------")
        print("Posez une autre question ou tapez 'exit' pour quitter.")
